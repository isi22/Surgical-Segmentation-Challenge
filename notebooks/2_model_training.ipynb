{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxdtCvC2mpR6"
      },
      "source": [
        "# Technical Challenge: Surgical Tool Segmentation\n",
        "\n",
        "This notebook implements a solution for the semantic segmentation of surgical tools from the SAR-RARP50 dataset. Semantic segmentation is a computer vision task that involves labelling every pixel in an image with a corresponding class.\n",
        "\n",
        "**Objective:** Generate pixel-level masks for RGB frames of surgical videos, segmenting both prominent surgical tools as well as thin and small objects such as surgical clips, suturing threads and needles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnHObcRfGgWk"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "This section prepares the environment. The notebook is designed to be run in Google Colab, but can be adapted to be run locally.\n",
        "\n",
        "The following code will:\n",
        "\n",
        "1. **Clone the Project Repository:** Pulls the necessary files and project structure from GitHub.\n",
        "\n",
        "2. **Install Dependencies:** Installs all required Python libraries from the requirements.txt file.\n",
        "\n",
        "*If you are running this notebook locally, you have likely already cloned the GitHub repository. The main setup step is to install the required Python libraries.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLLA7BRhGgWk"
      },
      "outputs": [],
      "source": [
        "# Folder location of the cloned or to-be-cloned repository\n",
        "PROJECT_DIR = \"Surgical-Segmentation-Challenge\"\n",
        "\n",
        "# --- COMMENT OUT IF RUNNING LOCALLY ---\n",
        "# This section is for setting up the environment in Google Colab\n",
        "%cd \"/content\"\n",
        "repo_url = \"https://github.com/isi22/Surgical-Segmentation-Challenge.git\"\n",
        "!git clone \"{repo_url}\" \"{PROJECT_DIR}\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# --- END COMMENT OUT ---\n",
        "\n",
        "# Change the current working directory to the project root\n",
        "%cd \"{PROJECT_DIR}\"\n",
        "\n",
        "# Install all required packages\n",
        "print(\"\\nInstalling dependencies from requirements.txt...\")\n",
        "!pip install -q -r requirements.txt\n",
        "\n",
        "print(\"\\n Environment setup complete. Current directory:\")\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWe0_rQM4JbC"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ7Syq6hytAu"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.patches as mpatches\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7pX-CKce_jF"
      },
      "source": [
        "## 3. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACltZWEtfBYV"
      },
      "outputs": [],
      "source": [
        "# --- Training Hyperparameters ---\n",
        "VAL_SPLIT = 0.2\n",
        "BATCH_SIZE = 8\n",
        "SEED = 42\n",
        "IMG_ROWS, IMG_COLS = 480, 640\n",
        "\n",
        "# --- Path Definitions ---\n",
        "DATA_DIR = 'data'\n",
        "MODEL_DIR = '/content/drive/MyDrive/Colab Notebooks/Surgical_Tool_Segmentation/data/models'\n",
        "TRAIN_PATH = os.path.join(DATA_DIR, 'train_dataset')\n",
        "TEST_PATH = os.path.join(DATA_DIR, 'test_dataset')\n",
        "MODEL_SAVE_PATH = os.path.join(MODEL_DIR, 'unet_efficientnetb4_best.h5')\n",
        "HISTORY_SAVE_PATH = os.path.join(MODEL_DIR, 'live_training_history.csv')\n",
        "\n",
        "# --- Model & Class Definitions ---\n",
        "input_shape = (IMG_ROWS, IMG_COLS, 3)\n",
        "class_names = [\n",
        "    \"Background\", \"Tool clasper\", \"Tool wrist\", \"Tool shaft\",\n",
        "    \"Suturing needle\", \"Thread\", \"Suction tool\", \"Needle holder\",\n",
        "    \"Clamps\", \"Catheter\"\n",
        "]\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrvVyHiQmn4J"
      },
      "source": [
        "## 4. Data Setup\n",
        "\n",
        "This section handles the download of the SAR-RARP50 dataset.\n",
        "\n",
        "The train and test data is stored as two pre-processed `.zip` files on Google Drive. The zip files were generated by the `1_data_preprocessing.ipynb` notebook, which handled the following pre-processing:\n",
        " - downloaded and unzipped the original archives\n",
        " - extracted images from the surgical videos that match the segmentation masks\n",
        " - consolidated all images and masks into flat `all_rgb` and `all_segmentation` directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akSHlYbC1BL9"
      },
      "source": [
        "### Download the Data\n",
        "\n",
        "**When using Google Colab:**\n",
        "\n",
        "When using Google Colab, the data needs to be re-downloaded every time a new virtual machine is used. The following cell automatically downloads the pre-processed data from the public Google Drive links."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeE9_EKT01fk"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "\n",
        "# # Create the data directory\n",
        "# os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# if not os.path.exists(TRAIN_PATH) or not os.path.exists(TEST_PATH):\n",
        "#     print(\"Data not found. Starting download and unzipping process...\")\n",
        "\n",
        "#     # Publicly shared Google Drive file IDs for the zip files\n",
        "#     train_id = \"1qqJbuOcBWa64oqNCRNNrz5663zhv1kVK\"\n",
        "#     test_id = \"1px81XhKGojwhSLVQTYUOeru5MWBoKzEO\"\n",
        "\n",
        "#     print(\"Downloading train.zip...\")\n",
        "#     !gdown --id {train_id} -O data/train_dataset.zip\n",
        "\n",
        "#     print(\"\\nDownloading test.zip...\")\n",
        "#     !gdown --id {test_id} -O data/test_dataset.zip\n",
        "\n",
        "#     # Unzip the files into the structured folders\n",
        "#     print(\"\\nUnzipping data...\")\n",
        "#     !unzip -q data/train_dataset.zip -d {TRAIN_PATH}\n",
        "#     !unzip -q data/test_dataset.zip -d {TEST_PATH}\n",
        "\n",
        "#     # Clean up the zip files\n",
        "#     !rm data/train_dataset.zip data/test_dataset.zip\n",
        "\n",
        "#     print(\"\\n Data download complete.\")\n",
        "# else:\n",
        "#     print(\"Data already downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1szu6fnSCvs"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Create the data directories\n",
        "os.makedirs(TRAIN_PATH, exist_ok=True)\n",
        "os.makedirs(TEST_PATH, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(os.path.join(TRAIN_PATH, 'all_rgb')) or not os.path.exists(os.path.join(TEST_PATH, 'all_rgb')):\n",
        "  print(\"Copying train_dataset.zip...\")\n",
        "  !cp \"/content/drive/MyDrive/Colab Notebooks/Surgical_Tool_Segmentation/data/test_dataset.zip\" \"data/\"\n",
        "  print(\"\\nCopying test_dataset.zip...\")\n",
        "  !cp \"/content/drive/MyDrive/Colab Notebooks/Surgical_Tool_Segmentation/data/train_dataset.zip\" \"data/\"\n",
        "\n",
        "  #Unzip the files into the structured folders\n",
        "  print(\"\\nUnzipping data...\")\n",
        "  !unzip -q data/train_dataset.zip -d {TRAIN_PATH}\n",
        "  !unzip -q data/test_dataset.zip -d {TEST_PATH}\n",
        "\n",
        "  # Clean up the zip files\n",
        "  !rm data/train_dataset.zip data/test_dataset.zip\n",
        "\n",
        "  print(\"\\n Data copy complete.\")\n",
        "else:\n",
        "    print(\"Data already copied.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL-ONxvPGgWl"
      },
      "source": [
        "**When running the notebook locally:**\n",
        "\n",
        "1. **Create the `data` Directory:** inside the `Surgical-Segmentation-Challenge` project folder, create a new folder named `data`.\n",
        "\n",
        "2. **Download the Zip Files:** download the two pre-processed zip files:\n",
        "\n",
        "   - Train Set: https://drive.google.com/file/d/1qqJbuOcBWa64oqNCRNNrz5663zhv1kVK/view?usp=drive_link\n",
        "\n",
        "   - Test Set: https://drive.google.com/file/d/1px81XhKGojwhSLVQTYUOeru5MWBoKzEO/view?usp=drive_link\n",
        "\n",
        "3. **Unzip the Data:** place the downloaded `training_dataset.zip` and `test_dataset.zip` files inside the data folder and unzip them.\n",
        "\n",
        "The final directory structure should look like this:\n",
        "\n",
        "```\n",
        "Surgical-Segmentation-Challenge/\n",
        "└── data/\n",
        "    └── train_dataset\n",
        "        └── all_segmentation\n",
        "        └── all_rgb\n",
        "    └── test_dataset\n",
        "        └── all_segmentation\n",
        "        └── all_rgb\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P6pnlTp08Yg"
      },
      "source": [
        "### Inspect the Data\n",
        "\n",
        "The dataset is already divided into train data (13043 frames from 40 videos, 80%) and test data (3252 from 10 videos, 20%). It has been sorted into the following directory structure, where the `all_rgb` folder contains the to-be-segmented images and the `all_segmentation` folder contains the ground-truth segmentation masks from all videos. From each of the 50 video segments, there are between 101 and 706 extracted frames available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XneXtPSG-qhF"
      },
      "outputs": [],
      "source": [
        "# Define folder locations\n",
        "all_train_rgb_path = os.path.join(TRAIN_PATH, 'all_rgb')\n",
        "all_train_seg_path = os.path.join(TRAIN_PATH, 'all_segmentation')\n",
        "all_test_rgb_path = os.path.join(TEST_PATH, 'all_rgb')\n",
        "all_test_seg_path = os.path.join(TEST_PATH, 'all_segmentation')\n",
        "\n",
        "# Function to get paths and verify files\n",
        "def get_and_verify_paths(dataset_rgb_path, dataset_seg_path):\n",
        "\n",
        "    image_paths = sorted(glob.glob(os.path.join(dataset_rgb_path, '*.png')))\n",
        "    mask_paths = sorted(glob.glob(os.path.join(dataset_seg_path, '*.png')))\n",
        "\n",
        "    # Get filenames without the full path\n",
        "    image_filenames = [os.path.basename(p) for p in image_paths]\n",
        "    mask_filenames = [os.path.basename(p) for p in mask_paths]\n",
        "\n",
        "    # Check for equal number of files\n",
        "    if len(image_paths) != len(mask_paths):\n",
        "        print(f\"Error: Mismatch in file counts.\")\n",
        "        print(f\"RGB images: {len(image_paths)}, Segmentation masks: {len(mask_paths)}\")\n",
        "\n",
        "    # Check if filenames are identical and in the same order\n",
        "    if image_filenames != mask_filenames:\n",
        "        print(f\"Error: Filenames do not match.\")\n",
        "\n",
        "    return image_paths, mask_paths\n",
        "\n",
        "# Get and verify paths for both datasets\n",
        "print(\"Verifying training data...\")\n",
        "train_val_images, train_val_masks = get_and_verify_paths(all_train_rgb_path, all_train_seg_path)\n",
        "\n",
        "print(\"Verifying testing data...\")\n",
        "test_images, test_masks = get_and_verify_paths(all_test_rgb_path, all_test_seg_path)\n",
        "\n",
        "print(f\"\\nAll files verified. Total images:\")\n",
        "print(f\"Found {len(train_val_images)} images and masks for training/validation.\")\n",
        "print(f\"Found {len(test_images)} images and masks for testing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J26ow45CDaZ"
      },
      "source": [
        "The segmentation masks are provided at the same resolution as the video frames, with the grayscale value of each pixel corresponding to one of the following semantic classes:\n",
        "\n",
        "| Label            | Class Name      |\n",
        "| -------------    | -------------   |\n",
        "| 0                | Background      |\n",
        "| 1                | Tool clasper    |\n",
        "| 2                | Tool wrist      |\n",
        "| 3                | Tool shaft      |\n",
        "| 4                | Suturing needle |\n",
        "| 5                | Thread          |\n",
        "| 6                | Suction tool    |\n",
        "| 7                | Needle holder   |\n",
        "| 8                | Clamps          |\n",
        "| 9                | Catheter        |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6B5e64AIzPg"
      },
      "source": [
        "## 5. Data Pipeline and Augmentation\n",
        "\n",
        "This section defines the custom data generator and the image augmentation pipeline. The augmentation strategy uses the `albumentations` library and is based on techniques used by successful teams in the SAR-RARP50 challenge paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ7-lFMIGgWm"
      },
      "source": [
        "### Augmentation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54Ei2J0oHKc5"
      },
      "outputs": [],
      "source": [
        "def get_augmentation_pipeline():\n",
        "    \"\"\"\n",
        "    Defines the augmentation pipeline using albumentations.\n",
        "    These transformations are applied to the image and mask simultaneously.\n",
        "    \"\"\"\n",
        "    transform = A.Compose([\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.4),\n",
        "        A.ShiftScaleRotate(\n",
        "            shift_limit=0.05,\n",
        "            scale_limit=0.1,\n",
        "            rotate_limit=15,\n",
        "            p=0.5,\n",
        "            interpolation=cv2.INTER_NEAREST, # Use nearest for masks\n",
        "            border_mode=cv2.BORDER_CONSTANT\n",
        "        ),\n",
        "    ])\n",
        "    return transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDBHd-DjGgWm"
      },
      "source": [
        "### Custom Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl3ZNJ8CCDaZ"
      },
      "outputs": [],
      "source": [
        "class SurgicalToolGenerator(Sequence):\n",
        "    \"\"\"Custom data generator\"\"\"\n",
        "    def __init__(self, image_paths, mask_paths, batch_size, image_size, augment=False):\n",
        "        super().__init__()\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.augment = augment\n",
        "        if self.augment:\n",
        "            self.augmentor = get_augmentation_pipeline()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches per epoch.\"\"\"\n",
        "        return len(self.image_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generates a batch of data.\"\"\"\n",
        "        batch_image_paths = self.image_paths[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_mask_paths = self.mask_paths[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch_images = np.zeros((self.batch_size, self.image_size[0], self.image_size[1], 3), dtype=np.float32)\n",
        "        batch_masks = np.zeros((self.batch_size, self.image_size[0], self.image_size[1]), dtype=np.uint8)\n",
        "\n",
        "        for i, (img_path, mask_path) in enumerate(zip(batch_image_paths, batch_mask_paths)):\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            # Resize\n",
        "            img_resized = cv2.resize(img, (self.image_size[1], self.image_size[0]))\n",
        "            mask_resized = cv2.resize(mask, (self.image_size[1], self.image_size[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "\n",
        "            # Augment\n",
        "            if self.augment:\n",
        "                augmented = self.augmentor(image=img_resized, mask=mask_resized)\n",
        "                img_resized = augmented['image']\n",
        "                mask_resized = augmented['mask']\n",
        "\n",
        "            # Normalise images\n",
        "            batch_images[i] = img_resized / 255.0\n",
        "            batch_masks[i] = mask_resized\n",
        "\n",
        "        return batch_images, batch_masks\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Shuffle data at the end of every epoch.\"\"\"\n",
        "        if self.augment:\n",
        "            combined = list(zip(self.image_paths, self.mask_paths))\n",
        "            random.shuffle(combined)\n",
        "            unzipped = list(zip(*combined))\n",
        "            self.image_paths = list(unzipped[0])\n",
        "            self.mask_paths = list(unzipped[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ_ttS7RGgWm"
      },
      "source": [
        "### Instantiate Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhUiLrb4CDaZ"
      },
      "outputs": [],
      "source": [
        "# Get training and testing paths\n",
        "def get_paths(dataset_path):\n",
        "    image_paths = sorted(glob.glob(os.path.join(dataset_path, 'all_rgb/*.png'), recursive=True))\n",
        "    mask_paths = sorted(glob.glob(os.path.join(dataset_path, 'all_segmentation/*.png'), recursive=True))\n",
        "    return image_paths, mask_paths\n",
        "\n",
        "train_val_images, train_val_masks = get_paths(TRAIN_PATH)\n",
        "test_images, test_masks = get_paths(TEST_PATH)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    train_val_images, train_val_masks, test_size=VAL_SPLIT, random_state=SEED\n",
        ")\n",
        "\n",
        "# Instantiate the generators\n",
        "train_generator = SurgicalToolGenerator(\n",
        "    image_paths=train_images,\n",
        "    mask_paths=train_masks,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_ROWS, IMG_COLS),\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "validation_generator = SurgicalToolGenerator(\n",
        "    image_paths=val_images,\n",
        "    mask_paths=val_masks,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_ROWS, IMG_COLS),\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "test_generator = SurgicalToolGenerator(\n",
        "    image_paths=test_images,\n",
        "    mask_paths=test_masks,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_ROWS, IMG_COLS),\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "print(f\"Found {len(train_images)} images for training.\")\n",
        "print(f\"Found {len(val_images)} images for validation.\")\n",
        "print(f\"Found {len(test_images)} images for testing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z_oJ9FsDzLm"
      },
      "source": [
        "### Visualise a Data Sample\n",
        "\n",
        "This cell displays a few sample images and masks from the generator to verify that the pipeline is working correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n34OGwJXzFEu"
      },
      "outputs": [],
      "source": [
        "def visualise_from_generator(generator, num_examples=3):\n",
        "    \"\"\"Visualises images and masks from a data generator.\"\"\"\n",
        "    # Get a colormap for the segmentation mask\n",
        "    colours = plt.get_cmap('tab10', NUM_CLASSES)\n",
        "    custom_cmap = mcolors.ListedColormap(colours.colors)\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(num_examples, 2, figsize=(10, 5 * num_examples))\n",
        "    fig.suptitle('Visualising Example Images and Segmentation Masks', fontsize=24, fontweight='bold')\n",
        "\n",
        "    # Get a single batch from the generator\n",
        "    sample_image_batch, sample_mask_batch = generator[0] # Get the first batch\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        image = sample_image_batch[i]\n",
        "        mask = sample_mask_batch[i]\n",
        "        ax[i, 0].imshow(image)\n",
        "        ax[i, 0].set_title(f'Image #{i+1}', fontsize=16)\n",
        "        ax[i, 1].imshow(mask, cmap=custom_cmap, vmin=0, vmax=NUM_CLASSES-1)\n",
        "        ax[i, 1].set_title(f'Segmentation Mask #{i+1}', fontsize=16)\n",
        "\n",
        "    # Add a legend to the figure\n",
        "    legend_patches = [mpatches.Patch(color=colours.colors[i], label=class_names[i]) for i in range(NUM_CLASSES)]\n",
        "    fig.legend(handles=legend_patches, bbox_to_anchor=(1.05, 0.7), loc='upper left', fontsize=12)\n",
        "\n",
        "    # Clean up the plot\n",
        "    for axis in ax.flat:\n",
        "        axis.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 0.96]) # Adjust layout to make space for the legend\n",
        "    plt.show()\n",
        "\n",
        "visualise_from_generator(train_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOe93FRMk3w"
      },
      "source": [
        "## 6. Model Architecture (U-Net with Pre-trained Encoder)\n",
        "\n",
        "This section defines the model architecture. A U-Net model is used, which is a powerful model for biomedical image segmentation. To improve performance and training speed, the standard U-Net encoder is replaced with a pre-trained `EfficientNetB4` model, leveraging features learned from the ImageNet dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HtSYSzxFMBh"
      },
      "source": [
        "### Encoder (From Scratch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoGZBIzs8Ln-"
      },
      "outputs": [],
      "source": [
        "def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
        "    \"\"\" Adds 2 convolutional layers with Batch Normalisation and ReLU activation.\n",
        "    This is the fundamental building block of the U-Net.\"\"\"\n",
        "\n",
        "    x = input_tensor\n",
        "    for i in range(2):\n",
        "      x = tf.keras.layers.Conv2D(filters=n_filters,\n",
        "                                kernel_size=(kernel_size, kernel_size),\n",
        "                                kernel_initializer='he_normal',\n",
        "                                padding='same')(x)\n",
        "      x = tf.keras.layers.BatchNormalization()(x)\n",
        "      x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):\n",
        "    \"\"\" Defines one downsampling block of the encoder.\n",
        "    It consists of a convolutional block followed by max-pooling and dropout.\"\"\"\n",
        "\n",
        "    f = conv2d_block(inputs, n_filters=n_filters)\n",
        "    p = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(f)\n",
        "    p = tf.keras.layers.Dropout(dropout)(p)\n",
        "\n",
        "    return f, p\n",
        "\n",
        "def bottleneck(inputs):\n",
        "  \"\"\" Defines the bottleneck convolutions that link the encoder and decoder.\"\"\"\n",
        "  bottle_neck = conv2d_block(inputs, n_filters=1024)\n",
        "  return bottle_neck\n",
        "\n",
        "def encoder(inputs):\n",
        "    \"\"\" Defines the complete encoder (downsampling path) of the U-Net.\"\"\"\n",
        "    f1, p1 = encoder_block(inputs, n_filters=64, dropout=0.3)\n",
        "    f2, p2 = encoder_block(p1, n_filters=128, dropout=0.3)\n",
        "    f3, p3 = encoder_block(p2, n_filters=256, dropout=0.3)\n",
        "    f4, p4 = encoder_block(p3, n_filters=512, dropout=0.3)\n",
        "\n",
        "    #Bottleneck\n",
        "    b1 = bottleneck(p4)\n",
        "\n",
        "    return b1, (f1, f2, f3, f4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIAH-zVXKq0O"
      },
      "source": [
        "### Encoder (Pre-trained EfficientNetB4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1DciTo6KnIc"
      },
      "outputs": [],
      "source": [
        "def pretrained_encoder(inputs):\n",
        "    \"\"\"\n",
        "    Creates a pre-trained EfficientNetB4 encoder and extracts feature maps\n",
        "    at different scales for the U-Net skip connections.\n",
        "    \"\"\"\n",
        "\n",
        "    # include_top=False: Excludes the final classification layer.\n",
        "    # weights='imagenet': Loads weights pre-trained on the ImageNet dataset.\n",
        "    encoder = EfficientNetB4(include_top=False, weights='imagenet', input_tensor=inputs)\n",
        "\n",
        "    # Capture the skip connections at different levels of the encoder\n",
        "    # The layers below were chosen to match the downsampling rates of a typical U-Net\n",
        "    skip_connections = [\n",
        "        inputs, # f1 480x640\n",
        "        encoder.get_layer('block2a_expand_activation').output, # f2 240x320\n",
        "        encoder.get_layer('block3a_expand_activation').output, # f3 120x160\n",
        "        encoder.get_layer('block4a_expand_activation').output, # f4 60x80\n",
        "    ]\n",
        "\n",
        "    # Bottleneck\n",
        "    b1 = encoder.get_layer('block6a_expand_activation').output # 30X40\n",
        "\n",
        "    return b1, skip_connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-__-6WcUa1s2"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XACX8TJh1oKd"
      },
      "outputs": [],
      "source": [
        "def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
        "    \"\"\" Adds 2 convolutional layers with Batch Normalisation and ReLU activation.\"\"\"\n",
        "\n",
        "    x = input_tensor\n",
        "    for i in range(2):\n",
        "      x = tf.keras.layers.Conv2D(filters=n_filters,\n",
        "                                kernel_size=(kernel_size, kernel_size),\n",
        "                                kernel_initializer='he_normal',\n",
        "                                padding='same')(x)\n",
        "      x = tf.keras.layers.BatchNormalization()(x)\n",
        "      x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=2, dropout=0.3):\n",
        "  \"\"\" Defines one upsampling block of the decoder. \"\"\"\n",
        "  u = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides=strides, padding='same')(inputs)\n",
        "  c = tf.keras.layers.concatenate([u, conv_output])\n",
        "  c = tf.keras.layers.Dropout(dropout)(c)\n",
        "  c = conv2d_block(c, n_filters, kernel_size=3)\n",
        "  return c\n",
        "\n",
        "def decoder(inputs, convs, output_channels):\n",
        "  \"\"\" Defines the complete decoder (upsampling path) of the U-Net. \"\"\"\n",
        "\n",
        "  f1, f2, f3, f4 = convs\n",
        "\n",
        "  # Upsampling block 1: from 1024 to 512 filters\n",
        "  c6 = decoder_block(inputs, f4, n_filters=512)\n",
        "  # Upsampling block 2: from 512 to 256 filters\n",
        "  c7 = decoder_block(c6, f3, n_filters=256)\n",
        "  # Upsampling block 3: from 256 to 128 filters\n",
        "  c8 = decoder_block(c7, f2, n_filters=128)\n",
        "  # Upsampling block 4: from 128 to 64 filters\n",
        "  c9 = decoder_block(c8, f1, n_filters=64)\n",
        "\n",
        "  # Final output layer\n",
        "  outputs = tf.keras.layers.Conv2D(output_channels, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAtWsYwGExtB"
      },
      "source": [
        "### Full Model Assembly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gE1jiz5u6Zg"
      },
      "outputs": [],
      "source": [
        "def unet(input_shape, output_channels):\n",
        "  \"\"\" Defines the complete U-Net model built from scratch\n",
        "  by connecting the encoder and decoder.\"\"\"\n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "  encoder_output, convs = encoder(inputs)\n",
        "  outputs = decoder(encoder_output, convs, output_channels=output_channels)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "# Instantiate the model\n",
        "model = unet(input_shape, NUM_CLASSES)\n",
        "\n",
        "# See the resulting model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZsfmDmuManV"
      },
      "outputs": [],
      "source": [
        "def unet_pretrained(input_shape, output_channels):\n",
        "  \"\"\" Defines a pre-trained version of the U-Net model by connecting the\n",
        "  pre-trained EfficientNetB4 encoder and decoder.\"\"\"\n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "  encoder_output, convs = pretrained_encoder(inputs)\n",
        "  outputs = decoder(encoder_output, convs, output_channels=output_channels)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "# Instantiate the model\n",
        "model_pretrained = unet_pretrained(input_shape, NUM_CLASSES)\n",
        "\n",
        "# See the resulting model architecture\n",
        "model_pretrained.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DGH_4T0VYn"
      },
      "source": [
        "## 7. Training the Model\n",
        "\n",
        "This section defines the loss function and compiles the model.\n",
        "\n",
        "* **Loss Function:** we use a combined **loss of Sparse Categorical Cross-entropy** and **Dice Loss**. This is a common strategy in segmentation tasks to balance pixel-wise accuracy with the overlap of segmented regions, which is particularly helpful for imbalanced classes.\n",
        "\n",
        "* **Metric:** we use **MeanIoU (Mean Intersection over Union)**, which provides a meaningful measure of how well the predicted segmentation masks overlap with the ground-truth masks.\n",
        "\n",
        "* **Callbacks:** Several callbacks are used to improve the training process, including `ModelCheckpoint` to save the best model, `EarlyStopping` to prevent overfitting, and `ReduceLROnPlateau` to adjust the learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fciS9HxLGgWn"
      },
      "source": [
        "### History Logger Helper Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMedoT4zGgWn"
      },
      "outputs": [],
      "source": [
        "class HistoryLogger(Callback):\n",
        "    \"\"\"\n",
        "    A Keras callback that logs the training history to a CSV file\n",
        "    at the end of each epoch.\n",
        "    \"\"\"\n",
        "    def __init__(self, filename='live_training_history.csv'):\n",
        "        super().__init__()\n",
        "        self.filename = filename\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        header = ['epoch'] + self.model.metrics_names\n",
        "        with open(self.filename, 'w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(header)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        row = [epoch + 1] + [logs.get(name) for name in self.model.metrics_names]\n",
        "        with open(self.filename, 'a', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZcBMKAYD8DT"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVh1cHzcD7QY"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
        "    \"\"\"Calculates the Dice loss, a measure of overlap between masks.\"\"\"\n",
        "    y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=NUM_CLASSES)\n",
        "    y_pred_f = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    # Flatten all dimensions except the class dimension\n",
        "    y_true_f_flat = tf.keras.layers.Flatten()(y_true_one_hot)\n",
        "    y_pred_f_flat = tf.keras.layers.Flatten()(y_pred)\n",
        "\n",
        "    intersection = tf.reduce_sum(y_true_f_flat * y_pred_f_flat, axis=-1)\n",
        "    union = tf.reduce_sum(y_true_f_flat, axis=-1) + tf.reduce_sum(y_pred_f_flat, axis=-1)\n",
        "\n",
        "    dice_coeff = (2. * intersection + smooth) / (union + smooth)\n",
        "    return 1.0 - dice_coeff\n",
        "\n",
        "def combined_loss(y_true, y_pred):\n",
        "    \"\"\"Combines Sparse Categorical Cross-entropy and Dice Loss.\"\"\"\n",
        "    cross_entropy = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "    cross_entropy_mean = tf.reduce_mean(cross_entropy, axis=[1, 2])\n",
        "    dice = dice_loss(y_true, y_pred)\n",
        "    return (0.5 * cross_entropy_mean) + (0.5 * dice)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IOU Metric"
      ],
      "metadata": {
        "id": "6tgjx9wlQZS1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEyXtFjCzZv5"
      },
      "outputs": [],
      "source": [
        "# # Create an instance of the MeanIoU metric\n",
        "# iou_metric = MeanIoU(num_classes=NUM_CLASSES, sparse_y_true=True, sparse_y_pred=False)\n",
        "\n",
        "# def custom_mean_iou(y_true, y_pred):\n",
        "#     \"\"\"\n",
        "#     A wrapper for the MeanIoU metric that handles potential shape issues.\n",
        "#     \"\"\"\n",
        "#     # Keras metrics expect the last dimension to be 1 for the labels\n",
        "#     y_true = tf.expand_dims(y_true, axis=-1)\n",
        "\n",
        "#     # Update the state of the metric\n",
        "#     iou_metric.update_state(y_true, y_pred)\n",
        "\n",
        "#     # Return the result\n",
        "#     return iou_metric.result()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrVhVCifGgWo"
      },
      "source": [
        "### Compile and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StKDH_B9t4SD"
      },
      "outputs": [],
      "source": [
        "# --- Compile the Model ---\n",
        "model_pretrained.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=combined_loss,\n",
        "    metrics=[MeanIoU(num_classes=NUM_CLASSES, sparse_y_true=True, sparse_y_pred=False)]\n",
        ")\n",
        "\n",
        "# --- Define Callbacks ---\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "model_checkpoint = ModelCheckpoint(MODEL_SAVE_PATH, monitor='val_mean_io_u', mode='max', save_best_only=True, verbose=1)\n",
        "early_stopper = EarlyStopping(patience=8, monitor='val_mean_io_u', mode='max', verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_mean_io_u', mode='max', factor=0.2, patience=4, min_lr=1e-6, verbose=1)\n",
        "history_logger = HistoryLogger(HISTORY_SAVE_PATH)\n",
        "\n",
        "# --- Start Training ---\n",
        "N_EPOCHS = 50\n",
        "\n",
        "model_history = model_pretrained.fit(\n",
        "    train_generator,\n",
        "    epochs=N_EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[model_checkpoint, early_stopper, reduce_lr, history_logger]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1ILv8HaGgWo"
      },
      "source": [
        "## 8. Inference and Evaluation\n",
        "\n",
        "This final section visualises the training history and demonstrates how to use the trained model for inference. It loads the best-performing weights saved during training, makes predictions on unseen test data, and visualises the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1pe61K6GgWo"
      },
      "outputs": [],
      "source": [
        "# --- Visualise Training History ---\n",
        "def plot_history_from_csv(filename='live_training_history.csv'):\n",
        "    \"\"\"\n",
        "    Loads a training history from a CSV file and plots the results.\n",
        "    \"\"\"\n",
        "    # Load the history from the CSV file\n",
        "    history_df = pd.read_csv(filename)\n",
        "\n",
        "    # Find the correct name for the MeanIoU metric key\n",
        "    iou_metric_key = [key for key in history_df.columns if 'iou' in key.lower() and 'val' not in key.lower()][0]\n",
        "    val_iou_metric_key = 'val_' + iou_metric_key\n",
        "\n",
        "    # --- Plot Loss ---\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_df['epoch'], history_df['loss'], label='Training Loss')\n",
        "    plt.plot(history_df['epoch'], history_df['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # --- Plot Mean IoU ---\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_df['epoch'], history_df[iou_metric_key], label='Training Mean IoU')\n",
        "    plt.plot(history_df['epoch'], history_df[val_iou_metric_key], label='Validation Mean IoU')\n",
        "    plt.title('Mean IoU Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Mean IoU')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nPlotting training history from the saved CSV file:\")\n",
        "plot_history_from_csv(HISTORY_SAVE_PATH)\n",
        "\n",
        "# --- Load the Best Saved Model ---\n",
        "print(f\"Loading best saved model from: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "# We pass the custom loss function to the load_model call\n",
        "best_model = load_model(MODEL_SAVE_PATH, custom_objects={'combined_loss': combined_loss})\n",
        "\n",
        "# --- Evaluate on the Test Set ---\n",
        "print(\"\\nEvaluating model performance on the test set...\")\n",
        "test_loss, test_iou = best_model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Mean IoU: {test_iou:.4f}\")\n",
        "\n",
        "# --- Visualise Predictions ---\n",
        "def visualise_predictions(model, generator, num_examples=3):\n",
        "    \"\"\"Makes predictions and visualises them against the ground truth.\"\"\"\n",
        "    colours = plt.get_cmap('tab10', NUM_CLASSES)\n",
        "    custom_cmap = mcolors.ListedColormap(colours.colors)\n",
        "    fig, ax = plt.subplots(num_examples, 3, figsize=(18, 5 * num_examples))\n",
        "    fig.suptitle('Model Predictions vs. Ground Truth', fontsize=20)\n",
        "\n",
        "    # Get a batch of test data\n",
        "    img_batch, mask_batch = generator[0]\n",
        "\n",
        "    # Make predictions\n",
        "    pred_batch_prob = model.predict(img_batch)\n",
        "    pred_batch = np.argmax(pred_batch_prob, axis=-1)\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        ax[i, 0].imshow(img_batch[i])\n",
        "        ax[i, 0].set_title(f'Input Image #{i+1}')\n",
        "\n",
        "        ax[i, 1].imshow(mask_batch[i], cmap=custom_cmap, vmin=0, vmax=NUM_CLASSES-1)\n",
        "        ax[i, 1].set_title(f'Ground Truth Mask')\n",
        "\n",
        "        ax[i, 2].imshow(pred_batch[i], cmap=custom_cmap, vmin=0, vmax=NUM_CLASSES-1)\n",
        "        ax[i, 2].set_title(f'Predicted Mask')\n",
        "\n",
        "        for axis in [ax[i, 0], ax[i, 1], ax[i, 2]]:\n",
        "            axis.axis('off')\n",
        "\n",
        "    legend_patches = [mpatches.Patch(color=colours.colors[i], label=class_names[i]) for i in range(NUM_CLASSES)]\n",
        "    fig.legend(handles=legend_patches, bbox_to_anchor=(1.1, 0.7), loc='upper right')\n",
        "    plt.tight_layout(rect=[0, 0, 0.95, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nVisualising predictions on a sample of the test set...\")\n",
        "visualise_predictions(best_model, test_generator)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}