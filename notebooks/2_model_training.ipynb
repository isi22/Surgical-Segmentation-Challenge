{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxdtCvC2mpR6"
      },
      "source": [
        "# Technical Challenge: Surgical Tool Segmentation\n",
        "\n",
        "This notebook implements semantic image segmentation of surgical tool parts. Semantic segmentation is a computer vision task that involves labelling every pixel in an image with a corresponding class.\n",
        "\n",
        "**Objective:** Generate pixel-level masks for RGB frames of surgical videos, segmenting both prominent surgical tools as well as thin and small objects such as surgical clips, suturing threads and needles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWe0_rQM4JbC"
      },
      "source": [
        "## Install and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ7Syq6hytAu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Check if still needed\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7pX-CKce_jF"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACltZWEtfBYV"
      },
      "outputs": [],
      "source": [
        "# --- Mount Google Drive to access the project files and dataset ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Configuration ---\n",
        "VAL_SPLIT = 0.2\n",
        "BATCH_SIZE = 16\n",
        "SEED = 42 # A random seed for reproducibility\n",
        "IMG_ROWS, IMG_COLS =  480, 640 # Per the README, resolution of all videos is 1080i, i.e. 1920, 1080, but we will use a smaller size for faster processing\n",
        "\n",
        "data_root = '/content'\n",
        "train_path = os.path.join(data_root, 'train_dataset')\n",
        "test_path = os.path.join(data_root, 'test_dataset')\n",
        "\n",
        "input_shape = (IMG_ROWS, IMG_COLS, 3)\n",
        "\n",
        "class_names = [\n",
        "    \"Background\",\n",
        "    \"Tool clasper\",\n",
        "    \"Tool wrist\",\n",
        "    \"Tool shaft\",\n",
        "    \"Suturing needle\",\n",
        "    \"Thread\",\n",
        "    \"Suction tool\",\n",
        "    \"Needle holder\",\n",
        "    \"Clamps\",\n",
        "    \"Catheter\"\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # --- Copy the compressed pre-processed data from my google drive to the local disk ---\n",
        "# train_zip_path = \"/content/drive/MyDrive/Colab Notebooks/Surgical_Tool_Segmentation/data/train_dataset.zip\"\n",
        "# !cp \"{train_zip_path}\" \"/content/\"\n",
        "# print(\"Train zip file copied.\")\n",
        "\n",
        "# test_zip_path = \"/content/drive/MyDrive/Colab Notebooks/Surgical_Tool_Segmentation/data/test_dataset.zip\"\n",
        "# !cp \"{test_zip_path}\" \"/content/\"\n",
        "# print(\"Test zip file copied.\")"
      ],
      "metadata": {
        "id": "MUpyGE_mq2Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# # --- Unzip the data on the local disk ---\n",
        "# !unzip -q \"/content/train_dataset.zip\" -d \"/content/train_dataset\"\n",
        "# !unzip -q \"/content/test_dataset.zip\" -d \"/content/test_dataset\"\n",
        "\n",
        "# print(\"Data unzipped.\")"
      ],
      "metadata": {
        "id": "62n43c9BHPis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrvVyHiQmn4J"
      },
      "source": [
        "## Load and Inspect Data\n",
        "\n",
        "The dataset is already divided into train data (13043 frames from 40 videos, 80%) and test data (3252 from 10 videos, 20%). It is provided in the following directory structure, where the `rgb` folder contains the to-be-segmented images and the `segmentation` folder contains the ground-truth segmentation masks. From each video segment there are between 101 and 706 extracted frames available.\n",
        "\n",
        "```\n",
        "data/\n",
        "└── train_dataset\n",
        "    └── video_XX\n",
        "        └── segmentation\n",
        "        └── rgb\n",
        "    └── ...\n",
        "└── test_dataset\n",
        "    └── video_XX\n",
        "        └── segmentation\n",
        "        └── rgb\n",
        "    └── ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUKqh6rkyhd9"
      },
      "outputs": [],
      "source": [
        "# --- Get a list of video directories ---\n",
        "all_paths = glob.glob(f'/content/drive/MyDrive/Colab Notebooks/Surgical_Tool_Segmentation/data/*_dataset/video_*')\n",
        "all_video_dirs = [path for path in all_paths if os.path.isdir(path)]\n",
        "all_video_dirs.sort() # Sort the list for consistent processing order\n",
        "\n",
        "# --- Count Files and Collect Data ---\n",
        "summary_data = []\n",
        "print(f\"Generating summary of available data...\\n\")\n",
        "\n",
        "for video_dir in all_video_dirs:\n",
        "    video_name = os.path.basename(video_dir)\n",
        "    segmentation_path = os.path.join(video_dir, 'segmentation')\n",
        "    images_path = os.path.join(video_dir, 'rgb')\n",
        "\n",
        "    segmentation_count = len(os.listdir(segmentation_path)) if os.path.exists(segmentation_path) else 0\n",
        "    images_count = len(os.listdir(images_path)) if os.path.exists(images_path) else 0\n",
        "\n",
        "    summary_data.append({\n",
        "        'Video': video_name,\n",
        "        'Segmentation_Masks': segmentation_count,\n",
        "        'RGB_Images': images_count,\n",
        "    })\n",
        "\n",
        "# --- Display the Summary Table ---\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "merged_df = summary_df.groupby(summary_df['Video'].str.extract(r'(video_\\d+)')[0]).sum(numeric_only=True).reset_index()\n",
        "merged_df = merged_df.rename(columns={'Video': 'base_video'})\n",
        "\n",
        "print(f\"Min frames per video: {merged_df.Segmentation_Masks.min()}\")\n",
        "print(f\"Max frames per video: {merged_df.Segmentation_Masks.max()}\")\n",
        "print(f\"Total num of training frames: {merged_df.head(40)['Segmentation_Masks'].sum()}\")\n",
        "print(f\"Total num of testing frames: {merged_df.tail(10)['Segmentation_Masks'].sum()}\\n\")\n",
        "display(merged_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6B5e64AIzPg"
      },
      "source": [
        "## Data Pipeline\n",
        "\n",
        "We will implement a custom data generator (`SurgicalToolGenerator`) which will be responsible for:\n",
        "1.  Locating corresponding image and mask pairs.\n",
        "2.  Loading them in batches to avoid loading the entire dataset into memory.\n",
        "3.  Applying resizing, normalisation, and data augmentation on the fly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J26ow45CDaZ"
      },
      "source": [
        "The segmentation masks are provided at the same resolution as the video frames, with the grayscale value of each pixel corresponding to one of the following semantic classes:\n",
        "\n",
        "| Label            | Class Name      |\n",
        "| -------------    | -------------   |\n",
        "| 0                | Background      |\n",
        "| 1                | Tool clasper    |\n",
        "| 2                | Tool wrist      |\n",
        "| 3                | Tool shaft      |\n",
        "| 4                | Suturing needle |\n",
        "| 5                | Thread          |\n",
        "| 6                | Suction tool    |\n",
        "| 7                | Needle holder   |\n",
        "| 8                | Clamps          |\n",
        "| 9                | Catheter        |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl3ZNJ8CCDaZ"
      },
      "outputs": [],
      "source": [
        "def apply_augmentation(image, mask):\n",
        "    \"\"\"Applies a set of augmentations to an image and its corresponding mask.\"\"\"\n",
        "    # Paired Augmentations (applied to both image and mask)\n",
        "    # Horizontal Flip\n",
        "    if random.random() > 0.5:\n",
        "        image = cv2.flip(image, 1)\n",
        "        mask = cv2.flip(mask, 1)\n",
        "\n",
        "    # Image-Only Augmentations\n",
        "    # Brightness/Contrast\n",
        "    if random.random() > 0.5:\n",
        "        # Adjust brightness by a random factor\n",
        "        brightness_factor = random.uniform(0.8, 1.2)\n",
        "        image = np.clip(image * brightness_factor, 0, 255).astype(np.uint8)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "class SurgicalToolGenerator(Sequence):\n",
        "    \"\"\"Custom data generator for surgical tool segmentation.\"\"\"\n",
        "    def __init__(self, image_paths, mask_paths, batch_size, image_size, augment=False):\n",
        "        super().__init__()\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch.\"\"\"\n",
        "        return len(self.image_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data.\"\"\"\n",
        "        batch_image_paths = self.image_paths[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        batch_mask_paths = self.mask_paths[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        batch_images = np.zeros((self.batch_size, self.image_size[0], self.image_size[1], 3), dtype=np.float32)\n",
        "        batch_masks = np.zeros((self.batch_size, self.image_size[0], self.image_size[1]), dtype=np.uint8)\n",
        "\n",
        "        for i, (img_path, mask_path) in enumerate(zip(batch_image_paths, batch_mask_paths)):\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            if mask is None:\n",
        "                print(f\"WARNING: Could not read mask {mask_path}. Filling with zeros.\")\n",
        "                mask = np.zeros((10, 10), dtype=np.uint8)\n",
        "\n",
        "            if self.augment:\n",
        "                img, mask = apply_augmentation(img, mask)\n",
        "\n",
        "            img_resized = cv2.resize(img, (self.image_size[1], self.image_size[0]))\n",
        "            mask_resized = cv2.resize(mask, (self.image_size[1], self.image_size[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            batch_images[i] = img_resized / 255.0\n",
        "            batch_masks[i] = mask_resized\n",
        "\n",
        "        return batch_images, batch_masks\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Shuffle data at the end of every epoch.\"\"\"\n",
        "        if self.augment:\n",
        "            combined = list(zip(self.image_paths, self.mask_paths))\n",
        "            random.shuffle(combined)\n",
        "            unzipped = list(zip(*combined))\n",
        "            self.image_paths = list(unzipped[0])\n",
        "            self.mask_paths = list(unzipped[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhUiLrb4CDaZ"
      },
      "outputs": [],
      "source": [
        "# --- Prepare file paths for generators ---\n",
        "def get_paths(dataset_path):\n",
        "    image_paths = sorted(glob.glob(os.path.join(dataset_path, 'all_rgb/*.png'), recursive=True))\n",
        "    mask_paths = sorted(glob.glob(os.path.join(dataset_path, 'all_segmentation/*.png'), recursive=True))\n",
        "    return image_paths, mask_paths\n",
        "\n",
        "train_val_images, train_val_masks = get_paths(train_path)\n",
        "test_images, test_masks = get_paths(test_path)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    train_val_images, train_val_masks, test_size=VAL_SPLIT, random_state=SEED\n",
        ")\n",
        "\n",
        "# --- Instantiate the generators ---\n",
        "train_generator = SurgicalToolGenerator(\n",
        "    image_paths=train_images,\n",
        "    mask_paths=train_masks,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_ROWS, IMG_COLS),\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "validation_generator = SurgicalToolGenerator(\n",
        "    image_paths=val_images,\n",
        "    mask_paths=val_masks,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_ROWS, IMG_COLS),\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "test_generator = SurgicalToolGenerator(\n",
        "    image_paths=test_images,\n",
        "    mask_paths=test_masks,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=(IMG_ROWS, IMG_COLS),\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "print(f\"Found {len(train_images)} images for training.\")\n",
        "print(f\"Found {len(val_images)} images for validation.\")\n",
        "print(f\"Found {len(test_images)} images for testing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z_oJ9FsDzLm"
      },
      "source": [
        "### Visualise Sample Images and Segmentation Masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n34OGwJXzFEu"
      },
      "outputs": [],
      "source": [
        "def visualise_from_generator(generator, num_examples, class_names):\n",
        "    \"\"\"Visualises images and masks from a data generator.\"\"\"\n",
        "    # Get a colormap for the segmentation mask\n",
        "    colours = plt.get_cmap('tab10', len(class_names))\n",
        "    custom_cmap = mcolors.ListedColormap(colours.colors)\n",
        "\n",
        "    # Create the plot\n",
        "    fig, ax = plt.subplots(num_examples, 2, figsize=(10, 5 * num_examples))\n",
        "    fig.suptitle('Visualising Example Images and Segmentation Masks', fontsize=24, fontweight='bold')\n",
        "\n",
        "    # Get a single batch from the generator\n",
        "    sample_image_batch, sample_mask_batch = generator[0] # Get the first batch\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        # Pick a sample from the batch\n",
        "        image = sample_image_batch[i]\n",
        "        mask = sample_mask_batch[i]\n",
        "\n",
        "        ax[i, 0].imshow(image)\n",
        "        ax[i, 0].set_title(f'Image #{i+1}', fontsize=16)\n",
        "        ax[i, 1].imshow(mask, cmap=custom_cmap, vmin=0, vmax=len(class_names)-1)\n",
        "        ax[i, 1].set_title(f'Segmentation Mask #{i+1}', fontsize=16)\n",
        "\n",
        "    # Add a legend to the figure\n",
        "    legend_patches = [mpatches.Patch(color=colours.colors[i], label=class_names[i]) for i in range(len(class_names))]\n",
        "    fig.legend(handles=legend_patches, bbox_to_anchor=(1.05, 0.7), loc='upper left', fontsize=12)\n",
        "\n",
        "    # Clean up the plot\n",
        "    for axis in ax.flat:\n",
        "        axis.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 0.96]) # Adjust layout to make space for the legend\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSkixpJvVVmH"
      },
      "outputs": [],
      "source": [
        "# Visualise a few examples from the training generator to check augmentations\n",
        "visualise_from_generator(train_generator, num_examples=3, class_names=class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOe93FRMk3w"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "We will use a **U-Net** architecture, which is the standard and a highly effective choice for biomedical image segmentation. Its key features are:\n",
        "\n",
        "1.  An **Encoder** (contracting path) that captures context by downsampling the image and extracting features at different scales.\n",
        "2.  A **Decoder** (expansive path) that uses transposed convolutions to upsample the feature maps, enabling precise localisation.\n",
        "3.  **Skip Connections** that merge feature maps from the encoder path with the corresponding decoder path. This is crucial as it allows the network to combine high-level contextual features with fine-grained spatial information, resulting in accurate segmentation masks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HtSYSzxFMBh"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJqbVK3FDUjK"
      },
      "source": [
        "The encoder consists of repeated blocks of convolutions followed by max-pooling to downsample the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoGZBIzs8Ln-"
      },
      "outputs": [],
      "source": [
        "# Encoder Utilities\n",
        "\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
        "  '''\n",
        "  Adds 2 convolutional layers with Batch Normalisation and ReLU activation.\n",
        "  This is the fundamental building block of the U-Net.\n",
        "\n",
        "  Args:\n",
        "    input_tensor (tensor): The input tensor.\n",
        "    n_filters (int): Number of filters for the convolutional layers.\n",
        "    kernel_size (int): Kernel size for the convolution.\n",
        "\n",
        "  Returns:\n",
        "    x (tensor): Tensor of output features.\n",
        "  '''\n",
        "  x = input_tensor\n",
        "  for _ in range(2):\n",
        "    x = tf.keras.layers.Conv2D(filters=n_filters,\n",
        "                              kernel_size=(kernel_size, kernel_size),\n",
        "                              kernel_initializer='he_normal',\n",
        "                              padding='same')(x)\n",
        "    # --- IMPROVEMENT: Added Batch Normalisation --- #\n",
        "    # This helps stabilise and accelerate training.\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "\n",
        "def encoder_block(inputs, n_filters=64, pool_size=(2,2), dropout=0.3):\n",
        "  '''\n",
        "  Defines one downsampling block of the encoder.\n",
        "  It consists of a convolutional block followed by max-pooling and dropout.\n",
        "\n",
        "  Returns:\n",
        "    f: The output features of the convolution block (for the skip connection).\n",
        "    p: The max-pooled features to be passed to the next block.\n",
        "  '''\n",
        "  f = conv2d_block(inputs, n_filters=n_filters)\n",
        "  p = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(f)\n",
        "  p = tf.keras.layers.Dropout(dropout)(p)\n",
        "\n",
        "  return f, p\n",
        "\n",
        "\n",
        "def encoder(inputs):\n",
        "  '''\n",
        "  Defines the complete encoder (downsampling path) of the U-Net.\n",
        "\n",
        "  Returns:\n",
        "    p4: The output features from the final encoder block (to be passed to the bottleneck).\n",
        "    (f1, f2, f3, f4): A tuple of feature maps from each encoder block for the skip connections.\n",
        "  '''\n",
        "  f1, p1 = encoder_block(inputs, n_filters=64, dropout=0.3)\n",
        "  f2, p2 = encoder_block(p1, n_filters=128, dropout=0.3)\n",
        "  f3, p3 = encoder_block(p2, n_filters=256, dropout=0.3)\n",
        "  f4, p4 = encoder_block(p3, n_filters=512, dropout=0.3)\n",
        "\n",
        "  return p4, (f1, f2, f3, f4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6lSYsoOc6j6"
      },
      "source": [
        "### Bottleneck\n",
        "\n",
        "\n",
        "A bottleneck block sits at the bottom of the \"U\" shape, between the encoder and decoder. It applies further convolutions to the feature map with the highest level of abstraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLzUf31Cuh-f"
      },
      "outputs": [],
      "source": [
        "def bottleneck(inputs):\n",
        "  '''\n",
        "  This function defines the bottleneck convolutions that link the encoder and decoder.\n",
        "  '''\n",
        "  bottle_neck = conv2d_block(inputs, n_filters=1024)\n",
        "  return bottle_neck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-__-6WcUa1s2"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder upsamples the feature maps back to the original image size. At each level, it merges the upsampled features with the corresponding high-resolution features from the encoder via a skip connection. This is the key mechanism that allows U-Net to produce highly detailed segmentation masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XACX8TJh1oKd"
      },
      "outputs": [],
      "source": [
        "# Decoder Utilities\n",
        "\n",
        "def decoder_block(inputs, conv_output, n_filters=64, kernel_size=3, strides=2, dropout=0.3):\n",
        "  '''\n",
        "  Defines one upsampling block of the decoder.\n",
        "  It uses a transposed convolution to upsample, concatenates with the skip connection,\n",
        "  and then applies a standard convolutional block.\n",
        "  '''\n",
        "  u = tf.keras.layers.Conv2DTranspose(n_filters, kernel_size, strides=strides, padding='same')(inputs)\n",
        "  c = tf.keras.layers.concatenate([u, conv_output])\n",
        "  c = tf.keras.layers.Dropout(dropout)(c)\n",
        "  c = conv2d_block(c, n_filters, kernel_size=3)\n",
        "  return c\n",
        "\n",
        "\n",
        "def decoder(inputs, convs, output_channels):\n",
        "  '''\n",
        "  Defines the complete decoder (upsampling path) of the U-Net.\n",
        "\n",
        "  Args:\n",
        "    inputs (tensor): Input features from the bottleneck.\n",
        "    convs (tuple): Feature maps from the encoder for skip connections.\n",
        "    output_channels (int): Number of classes for the final segmentation map.\n",
        "  '''\n",
        "  f1, f2, f3, f4 = convs\n",
        "\n",
        "  # Upsampling block 1: from 1024 to 512 filters\n",
        "  c6 = decoder_block(inputs, f4, n_filters=512)\n",
        "  # Upsampling block 2: from 512 to 256 filters\n",
        "  c7 = decoder_block(c6, f3, n_filters=256)\n",
        "  # Upsampling block 3: from 256 to 128 filters\n",
        "  c8 = decoder_block(c7, f2, n_filters=128)\n",
        "  # Upsampling block 4: from 128 to 64 filters\n",
        "  c9 = decoder_block(c8, f1, n_filters=64)\n",
        "\n",
        "  # Final output layer\n",
        "  outputs = tf.keras.layers.Conv2D(output_channels, (1, 1), activation='softmax')(c9)\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAtWsYwGExtB"
      },
      "source": [
        "### Putting It All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gE1jiz5u6Zg"
      },
      "outputs": [],
      "source": [
        "def unet(input_shape, output_channels):\n",
        "  '''\n",
        "  Defines the complete U-Net model by connecting the encoder, bottleneck, and decoder.\n",
        "  '''\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "  encoder_output, convs = encoder(inputs)\n",
        "  bottle_neck = bottleneck(encoder_output)\n",
        "  outputs = decoder(bottle_neck, convs, output_channels=output_channels)\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "# Instantiate the model\n",
        "model = unet(input_shape, NUM_CLASSES)\n",
        "\n",
        "# See the resulting model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DGH_4T0VYn"
      },
      "source": [
        "## Compile and Train the Model\n",
        "\n",
        "Now, we compile and train the model. Key choices here are:\n",
        "\n",
        "* **Loss Function:** We use `sparse_categorical_crossentropy` since our ground-truth masks are integer-encoded (0, 1, 2...) for each pixel, while our model outputs a probability distribution over the classes for each pixel (thanks to the softmax activation).\n",
        "\n",
        "* **Metric:** We use `MeanIoU` (Mean Intersection over Union), which provides a meaningful measure of how well the predicted segmentation masks overlap with the ground-truth masks.\n",
        "\n",
        "* **Callbacks:** We use several callbacks to improve the training process:\n",
        "    * `ModelCheckpoint`: Saves the best version of the model based on validation loss.\n",
        "    * `EarlyStopping`: Halts training if the validation loss does not improve for a set number of epochs, preventing overfitting.\n",
        "    * `ReduceLROnPlateau`: Decreases the learning rate if the training process stagnates, which can help the model escape local minima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEyXtFjCzZv5"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the MeanIoU metric\n",
        "iou_metric = MeanIoU(num_classes=NUM_CLASSES, sparse_y_true=True, sparse_y_pred=False)\n",
        "\n",
        "def custom_mean_iou(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    A wrapper for the MeanIoU metric that handles potential shape issues.\n",
        "    \"\"\"\n",
        "    # Keras metrics expect the last dimension to be 1 for the labels\n",
        "    y_true = tf.expand_dims(y_true, axis=-1)\n",
        "\n",
        "    # Update the state of the metric\n",
        "    iou_metric.update_state(y_true, y_pred)\n",
        "\n",
        "    # Return the result\n",
        "    return iou_metric.result()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=[custom_mean_iou]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StKDH_B9t4SD"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = 'surgical_tool_segmentation_best.h5'\n",
        "\n",
        "# Callback to save the best model\n",
        "model_checkpoint = ModelCheckpoint(checkpoint_path,\n",
        "                                 monitor='val_loss',\n",
        "                                 save_best_only=True,\n",
        "                                 mode='min',\n",
        "                                 verbose=1)\n",
        "\n",
        "# Callback to stop training early if validation loss stops improving\n",
        "early_stopper = EarlyStopping(patience=5,\n",
        "                              monitor='val_loss',\n",
        "                              mode='min',\n",
        "                              verbose=1)\n",
        "\n",
        "# Callback to reduce learning rate on plateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.2,\n",
        "                              patience=3,\n",
        "                              min_lr=0.00001,\n",
        "                              verbose=1)\n",
        "\n",
        "N_EPOCHS = 50\n",
        "\n",
        "model_history = model.fit(train_generator,\n",
        "                          epochs=N_EPOCHS,\n",
        "                          validation_data=validation_generator,\n",
        "                          callbacks=[model_checkpoint, early_stopper, reduce_lr]\n",
        "                          )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}